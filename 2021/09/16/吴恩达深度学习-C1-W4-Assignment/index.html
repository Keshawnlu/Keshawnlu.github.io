<!DOCTYPE html>
<html lang="">
    <!-- title -->




<!-- keywords -->




<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="John Doe">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="John Doe">
    
    <meta name="keywords" content="鸣蜩十九,hexo-theme,hexo-blog">
    
    <meta name="description" content>
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>吴恩达深度学习 C1_W4_Assignment · Keshawn_lu&#39;s Blog</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href="/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="stylesheet" href="/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href="/assets/头像.ico">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Keshawn_lu&#39;s Blog</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">吴恩达深度学习 C1_W4_Assignment</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Keshawn_lu's Blog</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://pic.superbed.cn/item/5d383e5d451253d178a78147.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            吴恩达深度学习 C1_W4_Assignment
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "吴恩达深度学习">吴恩达深度学习</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>字数统计: <span class="post-count word-count">3.2k</span>阅读时长: <span class="post-count reading-time">18 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2021/09/16</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="吴恩达深度学习-C1-W4-Assignment"><a href="#吴恩达深度学习-C1-W4-Assignment" class="headerlink" title="吴恩达深度学习 C1_W4_Assignment"></a>吴恩达深度学习 C1_W4_Assignment</h1><h2 id="任务1：一步一步构建深层神经网络"><a href="#任务1：一步一步构建深层神经网络" class="headerlink" title="任务1：一步一步构建深层神经网络"></a>任务1：一步一步构建深层神经网络</h2><h2 id="Part0：库的准备"><a href="#Part0：库的准备" class="headerlink" title="Part0：库的准备"></a>Part0：库的准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>步骤图：</strong></p>
<p><img src="https://pic.imgdb.cn/item/6135d3df44eaada7399c4856.png" alt="https://pic.imgdb.cn/item/6135d3df44eaada7399c4856.png"></p>
<h2 id="Part1：初始化"><a href="#Part1：初始化" class="headerlink" title="Part1：初始化"></a>Part1：初始化</h2><h3 id="Exercise1：创建并初始化2层神经网络参数"><a href="#Exercise1：创建并初始化2层神经网络参数" class="headerlink" title="Exercise1：创建并初始化2层神经网络参数"></a>Exercise1：创建并初始化2层神经网络参数</h3><p><strong>模型结构：LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h3 id="Exercise2：实现L层神经网络的初始化"><a href="#Exercise2：实现L层神经网络的初始化" class="headerlink" title="Exercise2：实现L层神经网络的初始化"></a>Exercise2：实现L层神经网络的初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="number">1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h2 id="Part2：前向传播模块"><a href="#Part2：前向传播模块" class="headerlink" title="Part2：前向传播模块"></a>Part2：前向传播模块</h2><p>接下来将完成三个函数：</p>
<ul>
<li>LINEAR</li>
<li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)</li>
</ul>
<h3 id="Exercise3：完成Linear的前向传播"><a href="#Exercise3：完成Linear的前向传播" class="headerlink" title="Exercise3：完成Linear的前向传播"></a>Exercise3：完成Linear的前向传播</h3><script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\A^{[0]} = X</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    Z = np.dot(W, A) + b</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure>
<h3 id="Exercise4：完成LINEAR-gt-ACTIVATION的前向传播"><a href="#Exercise4：完成LINEAR-gt-ACTIVATION的前向传播" class="headerlink" title="Exercise4：完成LINEAR-&gt;ACTIVATION的前向传播"></a>Exercise4：完成LINEAR-&gt;ACTIVATION的前向传播</h3><script type="math/tex; mode=display">A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})</script><p>其中<code>g()</code> 为<code>sigmoid</code>或<code>relu</code> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<h3 id="Exercise5：实现L层模型的前向传播"><a href="#Exercise5：实现L层模型的前向传播" class="headerlink" title="Exercise5：实现L层模型的前向传播"></a>Exercise5：实现L层模型的前向传播</h3><p><img src="https://pic.imgdb.cn/item/6135d86044eaada739a41659.png" alt="https://pic.imgdb.cn/item/6135d86044eaada739a41659.png"></p>
<script type="math/tex; mode=display">A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    <span class="comment"># 整除</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>    <span class="comment"># number of layers in the neural network</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">"W"</span> + str(l)],  parameters[<span class="string">"b"</span> + str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">"W"</span> + str(L)],  parameters[<span class="string">"b"</span> + str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure>
<h2 id="Part3：成本函数"><a href="#Part3：成本函数" class="headerlink" title="Part3：成本函数"></a>Part3：成本函数</h2><h3 id="Exercise6：计算成本函数"><a href="#Exercise6：计算成本函数" class="headerlink" title="Exercise6：计算成本函数"></a>Exercise6：计算成本函数</h3><script type="math/tex; mode=display">-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></span><br><span class="line">    cost = <span class="number">-1</span> / m * np.sum(np.dot(np.log(AL), Y.T) + np.dot(np.log(<span class="number">1</span> - AL), (<span class="number">1</span> - Y).T))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 将为1的维度去除 (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="Part4：反向传播模块"><a href="#Part4：反向传播模块" class="headerlink" title="Part4：反向传播模块"></a>Part4：反向传播模块</h2><p><img src="https://pic.imgdb.cn/item/6135dac644eaada739a7f3dd.png" alt="https://pic.imgdb.cn/item/6135dac644eaada739a7f3dd.png"></p>
<p>和前向传播一样，我们需要逐步完成三个函数：</p>
<ul>
<li>LINEAR backward</li>
<li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li>
</ul>
<h3 id="Exercise7：Linear-backward"><a href="#Exercise7：Linear-backward" class="headerlink" title="Exercise7：Linear backward"></a>Exercise7：Linear backward</h3><p><img src="https://pic.imgdb.cn/item/6135db6844eaada739a90752.png" alt="https://pic.imgdb.cn/item/6135db6844eaada739a90752.png"></p>
<script type="math/tex; mode=display">dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}\\db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\\dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    dW = <span class="number">1</span> / m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span> / m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) <span class="comment"># 每一行的值加起来</span></span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="Exercise8：Linear-Activation-backward"><a href="#Exercise8：Linear-Activation-backward" class="headerlink" title="Exercise8：Linear-Activation backward"></a>Exercise8：Linear-Activation backward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="Exercise9：L-Model-Backward"><a href="#Exercise9：L-Model-Backward" class="headerlink" title="Exercise9：L-Model Backward"></a>Exercise9：L-Model Backward</h3><p><img src="https://pic.imgdb.cn/item/6136244f44eaada73927c286.png" alt="https://pic.imgdb.cn/item/6136244f44eaada73927c286.png"></p>
<script type="math/tex; mode=display">dAL = \frac{\partial \mathcal{L}}{\partial A^{[L]}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">    cache_now = caches[L - <span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, cache_now, <span class="string">"sigmoid"</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L - <span class="number">1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">        cache_now = caches[l]</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] , grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] , grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">2</span>)], cache_now, <span class="string">"relu"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<h3 id="Exercise10：更新参数"><a href="#Exercise10：更新参数" class="headerlink" title="Exercise10：更新参数"></a>Exercise10：更新参数</h3><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(i)] -= learning_rate * grads[<span class="string">"dW"</span> + str(i)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(i)] -= learning_rate * grads[<span class="string">"db"</span> + str(i)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h2 id="任务2：深度神经网络在图像分类中的应用"><a href="#任务2：深度神经网络在图像分类中的应用" class="headerlink" title="任务2：深度神经网络在图像分类中的应用"></a>任务2：深度神经网络在图像分类中的应用</h2><h2 id="Part0：库的准备-1"><a href="#Part0：库的准备-1" class="headerlink" title="Part0：库的准备"></a>Part0：库的准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Part1：数据准备"><a href="#Part1：数据准备" class="headerlink" title="Part1：数据准备"></a>Part1：数据准备</h2><p>我们将使用与实验2相同的数据集，并来观察此次的模型是否能提高识别准确率</p>
<p>我们来观察一下测试集与训练集的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: <span class="number">209</span></span><br><span class="line">Number of testing examples: <span class="number">50</span></span><br><span class="line">Each image <span class="keyword">is</span> of size: (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_x_orig shape: (<span class="number">209</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">train_y shape: (<span class="number">1</span>, <span class="number">209</span>)</span><br><span class="line">test_x_orig shape: (<span class="number">50</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">test_y shape: (<span class="number">1</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>我们将图像向量化：</p>
<p><img src="https://pic.imgdb.cn/item/6136286144eaada739312174.png" alt="https://pic.imgdb.cn/item/6136286144eaada739312174.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples </span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T   <span class="comment"># 一个样本一列</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_x<span class="string">'s shape: (12288, 209)   # 12288 = 64 * 64 * 3</span></span><br><span class="line"><span class="string">test_x'</span>s shape: (<span class="number">12288</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Part2：模型结构"><a href="#Part2：模型结构" class="headerlink" title="Part2：模型结构"></a>Part2：模型结构</h2><p>我们将构建两种不同的模型：</p>
<ul>
<li>2层神经网络</li>
<li>L层深度神经网络</li>
</ul>
<h3 id="Exercise11：2层神经网络"><a href="#Exercise11：2层神经网络" class="headerlink" title="Exercise11：2层神经网络"></a>Exercise11：2层神经网络</h3><p><img src="https://pic.imgdb.cn/item/61362a0d44eaada739350ef6.png" alt="https://pic.imgdb.cn/item/61362a0d44eaada739350ef6.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3  (64 * 64 * 3)</span></span><br><span class="line">n_h = <span class="number">7</span>         <span class="comment"># the size of the hidden layer</span></span><br><span class="line">n_y = <span class="number">1</span>         <span class="comment"># the size of the output layer</span></span><br><span class="line"></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                              <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                          <span class="comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">"relu"</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>测试：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/61362b7144eaada739384fb8.png" alt="https://pic.imgdb.cn/item/61362b7144eaada739384fb8.png"></p>
<p>通过准确率的测试，可以得到结果为72%。</p>
<h3 id="Exercise12：L层神经网络"><a href="#Exercise12：L层神经网络" class="headerlink" title="Exercise12：L层神经网络"></a>Exercise12：L层神经网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>测试：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/61362d1544eaada7393c247f.png" alt="https://pic.imgdb.cn/item/61362d1544eaada7393c247f.png"></p>
<p>通过准确率的测试，可以得到结果为80%。由此可以得到结果，在相同的测试集上，5层神经网络比2层神经网络效果更好。</p>
<h2 id="Part3：错误结果分析"><a href="#Part3：错误结果分析" class="headerlink" title="Part3：错误结果分析"></a>Part3：错误结果分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/61362dd744eaada7393ddcf0.png" alt="https://pic.imgdb.cn/item/61362dd744eaada7393ddcf0.png"></p>
<p>该模型往往表现不佳的几种图像类型包括：</p>
<ul>
<li>猫的身体处于一个不寻常的位置</li>
<li>猫在相似颜色的背景下出现</li>
<li>不寻常的猫的颜色和种类</li>
<li>摄像机角度</li>
<li>画面的亮度</li>
<li>比例变化（图像中猫很大或很小）</li>
</ul>

    </article>
    <!-- license  -->
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2021/09/16/吴恩达深度学习-C2-W1-Assignment/" title= "吴恩达深度学习 C2_W1_Assignment">
                    <div class="nextTitle">吴恩达深度学习 C2_W1_Assignment</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2021/09/16/吴恩达深度学习-C1-W3-Assignment/" title= "吴恩达深度学习 C1_W3_Assignment">
                    <div class="prevTitle">吴恩达深度学习 C1_W3_Assignment</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

    <div id="lv-container" data-id="city" data-uid="MTAyMC80NTMxOS8yMTgzMg==">
        <script type="text/javascript">
            (function (d, s) {
                var j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') { return; }
                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    </div>

<!-- City版安装代码已完成 -->
    
    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:809759109@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/Keshawnlu" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                  
                  <img class="profile-qr" src="/assets/wechat.jpg" />
                </span>
            
        
    
        
            
                <span class="iconfont-archer qq" title=qq>
                  
                  <img class="profile-qr" src="/assets/qq.jpg" />
                </span>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">访问量为 <span id="busuanzi_value_site_pv"></span> 次啦!!</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#吴恩达深度学习-C1-W4-Assignment"><span class="toc-number">1.</span> <span class="toc-text">吴恩达深度学习 C1_W4_Assignment</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#任务1：一步一步构建深层神经网络"><span class="toc-number">1.1.</span> <span class="toc-text">任务1：一步一步构建深层神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part0：库的准备"><span class="toc-number">1.2.</span> <span class="toc-text">Part0：库的准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part1：初始化"><span class="toc-number">1.3.</span> <span class="toc-text">Part1：初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise1：创建并初始化2层神经网络参数"><span class="toc-number">1.3.1.</span> <span class="toc-text">Exercise1：创建并初始化2层神经网络参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise2：实现L层神经网络的初始化"><span class="toc-number">1.3.2.</span> <span class="toc-text">Exercise2：实现L层神经网络的初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part2：前向传播模块"><span class="toc-number">1.4.</span> <span class="toc-text">Part2：前向传播模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise3：完成Linear的前向传播"><span class="toc-number">1.4.1.</span> <span class="toc-text">Exercise3：完成Linear的前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise4：完成LINEAR-gt-ACTIVATION的前向传播"><span class="toc-number">1.4.2.</span> <span class="toc-text">Exercise4：完成LINEAR-&gt;ACTIVATION的前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise5：实现L层模型的前向传播"><span class="toc-number">1.4.3.</span> <span class="toc-text">Exercise5：实现L层模型的前向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part3：成本函数"><span class="toc-number">1.5.</span> <span class="toc-text">Part3：成本函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise6：计算成本函数"><span class="toc-number">1.5.1.</span> <span class="toc-text">Exercise6：计算成本函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part4：反向传播模块"><span class="toc-number">1.6.</span> <span class="toc-text">Part4：反向传播模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise7：Linear-backward"><span class="toc-number">1.6.1.</span> <span class="toc-text">Exercise7：Linear backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise8：Linear-Activation-backward"><span class="toc-number">1.6.2.</span> <span class="toc-text">Exercise8：Linear-Activation backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise9：L-Model-Backward"><span class="toc-number">1.6.3.</span> <span class="toc-text">Exercise9：L-Model Backward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise10：更新参数"><span class="toc-number">1.6.4.</span> <span class="toc-text">Exercise10：更新参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#任务2：深度神经网络在图像分类中的应用"><span class="toc-number">1.7.</span> <span class="toc-text">任务2：深度神经网络在图像分类中的应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part0：库的准备-1"><span class="toc-number">1.8.</span> <span class="toc-text">Part0：库的准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part1：数据准备"><span class="toc-number">1.9.</span> <span class="toc-text">Part1：数据准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part2：模型结构"><span class="toc-number">1.10.</span> <span class="toc-text">Part2：模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise11：2层神经网络"><span class="toc-number">1.10.1.</span> <span class="toc-text">Exercise11：2层神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercise12：L层神经网络"><span class="toc-number">1.10.2.</span> <span class="toc-text">Exercise12：L层神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part3：错误结果分析"><span class="toc-number">1.11.</span> <span class="toc-text">Part3：错误结果分析</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 276
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2022 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span><a class="archive-post-title" href= "/2022/07/07/Leetcode-648-单词替换/" >Leetcode.648 单词替换</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2021 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2021/09/16/吴恩达深度学习-C1-W2-Assignment/" >吴恩达深度学习 C1_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2021/09/16/吴恩达深度学习-C1-W4-Assignment/" >吴恩达深度学习 C1_W4_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2021/09/16/吴恩达深度学习-C1-W3-Assignment/" >吴恩达深度学习 C1_W3_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2021/09/16/吴恩达深度学习-C2-W1-Assignment/" >吴恩达深度学习 C2_W1_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2021/09/16/吴恩达深度学习-C2-W2-Assignment/" >吴恩达深度学习 C2_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C3-W1-Assignment/" >吴恩达团队NLP C3_W1_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C3-W4-Assignment/" >吴恩达团队NLP C3_W4_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C3-W3-Assignment/" >吴恩达团队NLP C3_W3_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C4-W1-Assignment/" >吴恩达团队NLP C4_W1_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C3-W2-Assignment/" >吴恩达团队NLP C3_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2021/08/28/吴恩达团队NLP-C4-W2-Assignment/" >吴恩达团队NLP C4_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2021/08/08/吴恩达团队NLP-C1-W4-Assignment/" >吴恩达团队NLP C1_W4_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2021/08/08/吴恩达团队NLP-C2-W1-Assignment/" >吴恩达团队NLP C2_W1_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2021/08/08/吴恩达团队NLP-C2-W3-Assignment/" >吴恩达团队NLP C2_W3_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2021/08/08/吴恩达团队NLP-C2-W2-Assignment/" >吴恩达团队NLP C2_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2021/08/08/吴恩达团队NLP-C2-W4-Assignment/" >吴恩达团队NLP C2_W4_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span><a class="archive-post-title" href= "/2021/07/25/吴恩达团队NLP-C1-W1-Assignment/" >吴恩达团队NLP C1_W1_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span><a class="archive-post-title" href= "/2021/07/25/吴恩达团队NLP-C1-W2-Assignment/" >吴恩达团队NLP C1_W2_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span><a class="archive-post-title" href= "/2021/07/25/吴恩达团队NLP-C1-W3-Assignment/" >吴恩达团队NLP C1_W3_Assignment</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span><a class="archive-post-title" href= "/2021/03/03/牛客网-KY52-位操作练习/" >牛客网 KY52.位操作练习</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span><a class="archive-post-title" href= "/2021/03/03/牛客网-KY68-子串计算/" >牛客网 KY68.子串计算</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span><a class="archive-post-title" href= "/2021/03/03/牛客网-KY73-合唱队形/" >牛客网 KY73.合唱队形</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span><a class="archive-post-title" href= "/2021/03/03/牛客网-KY8-整数拆分/" >牛客网 KY8.整数拆分</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span><a class="archive-post-title" href= "/2021/03/03/牛客网-KY41-放苹果/" >牛客网 KY41.放苹果</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/20</span><a class="archive-post-title" href= "/2021/02/20/PAT-1060-爱丁顿数/" >PAT 1060.爱丁顿数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/20</span><a class="archive-post-title" href= "/2021/02/20/PAT-1065-单身狗/" >PAT 1065.单身狗</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/20</span><a class="archive-post-title" href= "/2021/02/20/PAT-1067-试密码/" >PAT 1067.试密码</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/14</span><a class="archive-post-title" href= "/2021/02/14/PAT-1054-求平均值/" >PAT 1054.求平均值</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/13</span><a class="archive-post-title" href= "/2021/02/13/PAT-1045-快速排序/" >PAT 1045.快速排序</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/13</span><a class="archive-post-title" href= "/2021/02/13/PAT-1049-数列的片段和/" >PAT 1049.数列的片段和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/10</span><a class="archive-post-title" href= "/2021/02/10/PAT-1040-有几个PAT/" >PAT 1040.有几个PAT</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/07</span><a class="archive-post-title" href= "/2021/02/07/PAT-1033-旧键盘打字/" >PAT 1033.旧键盘打字</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/06</span><a class="archive-post-title" href= "/2021/02/06/PAT-1028-人口普查/" >PAT 1028.人口普查</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/05</span><a class="archive-post-title" href= "/2021/02/05/PAT-1019-数字黑洞/" >PAT 1019.数字黑洞</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/05</span><a class="archive-post-title" href= "/2021/02/05/PAT-1020-月饼/" >PAT 1020.月饼</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/2021/02/04/PAT-1017-A除以B/" >PAT 1017.A除以B</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/2021/02/04/PAT-1010-一元多项式求导/" >PAT 1010.一元多项式求导</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/03</span><a class="archive-post-title" href= "/2021/02/03/PAT-1005-继续-3n-1-猜想/" >PAT 1005.继续(3n+1)猜想</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/03</span><a class="archive-post-title" href= "/2021/02/03/PAT-1009-说反话/" >PAT 1009.说反话</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/03</span><a class="archive-post-title" href= "/2021/02/03/PAT-1008-数组元素循环右移问题/" >PAT 1008.数组元素循环右移问题</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/03</span><a class="archive-post-title" href= "/2021/02/03/PAT-1004-成绩排名/" >PAT 1004.成绩排名</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/02</span><a class="archive-post-title" href= "/2021/02/02/PAT-1003-我要通过/" >PAT 1003 我要通过</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/02</span><a class="archive-post-title" href= "/2021/02/02/浙工商OJ-5-jlh吃水果/" >浙工商OJ 5.jlh吃水果</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/02</span><a class="archive-post-title" href= "/2021/02/02/浙工商OJ-6-jlh的童年1/" >浙工商OJ 6.jlh的童年1</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span><a class="archive-post-title" href= "/2021/01/11/数据挖掘期末复习汇总/" >数据挖掘期末复习汇总</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2020 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/29</span><a class="archive-post-title" href= "/2020/11/29/Leetcode-976-三角形的最大周长/" >Leetcode 976. 三角形的最大周长</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/25</span><a class="archive-post-title" href= "/2020/11/25/Leetcode-1370-上升下降字符串/" >Leetcode 1370. 上升下降字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/24</span><a class="archive-post-title" href= "/2020/11/24/Leetcode-222-完全二叉树的节点个数/" >Leetcode 222. 完全二叉树的节点个数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span><a class="archive-post-title" href= "/2020/11/23/Leetcode-452-用最少数量的箭引爆气球/" >Leetcode 452. 用最少数量的箭引爆气球</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/22</span><a class="archive-post-title" href= "/2020/11/22/Leetcode-242-有效的字母异位词/" >Leetcode 242. 有效的字母异位词</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span><a class="archive-post-title" href= "/2020/11/20/Leetcode-1030-距离顺序排列矩阵单元格/" >Leetcode 1030. 距离顺序排列矩阵单元格</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span><a class="archive-post-title" href= "/2020/11/20/Leetcode-134-加油站/" >Leetcode 134. 加油站</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span><a class="archive-post-title" href= "/2020/11/20/Leetcode-147-对链表进行插入排序/" >Leetcode 147. 对链表进行插入排序</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span><a class="archive-post-title" href= "/2020/11/20/Leetcode-283-移动零/" >Leetcode 283. 移动零</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span><a class="archive-post-title" href= "/2020/11/20/Leetcode-406-根据身高重建队列/" >Leetcode 406. 根据身高重建队列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/15</span><a class="archive-post-title" href= "/2020/11/15/Leetcode-402-移掉K位数字/" >Leetcode 402. 移掉K位数字</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/14</span><a class="archive-post-title" href= "/2020/11/14/Leetcode-1122-数组的相对排序/" >Leetcode 1122. 数组的相对排序</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/14</span><a class="archive-post-title" href= "/2020/11/14/Leetcode-328-奇偶链表/" >Leetcode 328. 奇偶链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/14</span><a class="archive-post-title" href= "/2020/11/14/Leetcode-922-按奇偶排序数组-II/" >Leetcode 922. 按奇偶排序数组 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/10</span><a class="archive-post-title" href= "/2020/11/10/Leetcode-31-下一个排列/" >Leetcode 31. 下一个排列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/09</span><a class="archive-post-title" href= "/2020/11/09/Leetcode-122-买卖股票的最佳时机-II/" >Leetcode 122. 买卖股票的最佳时机 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/09</span><a class="archive-post-title" href= "/2020/11/09/Leetcode-127-单词接龙/" >Leetcode 127. 单词接龙</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/09</span><a class="archive-post-title" href= "/2020/11/09/Leetcode-973-最接近原点的-K-个点/" >Leetcode 973. 最接近原点的 K 个点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2020/11/03/Leetcode-349-两个数组的交集/" >Leetcode 349. 两个数组的交集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2020/11/03/Leetcode-463-岛屿的周长/" >Leetcode 463. 岛屿的周长</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2020/11/03/Leetcode-941-有效的山脉数组/" >Leetcode 941. 有效的山脉数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/29</span><a class="archive-post-title" href= "/2020/10/29/Leetcode-129-求根到叶子节点数字之和/" >Leetcode 129. 求根到叶子节点数字之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/28</span><a class="archive-post-title" href= "/2020/10/28/Leetcode-1207-独一无二的出现次数/" >Leetcode 1207. 独一无二的出现次数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/27</span><a class="archive-post-title" href= "/2020/10/27/Leetcode-1365-有多少小于当前数字的数字/" >Leetcode 1365. 有多少小于当前数字的数字</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/27</span><a class="archive-post-title" href= "/2020/10/27/Leetcode-845-数组中的最长山脉/" >Leetcode 845. 数组中的最长山脉</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/24</span><a class="archive-post-title" href= "/2020/10/24/Leetcode-1024-视频拼接/" >Leetcode 1024. 视频拼接</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/23</span><a class="archive-post-title" href= "/2020/10/23/Leetcode-234-回文链表/" >Leetcode 234. 回文链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/22</span><a class="archive-post-title" href= "/2020/10/22/Leetcode-763-划分字母区间/" >Leetcode 763. 划分字母区间</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/21</span><a class="archive-post-title" href= "/2020/10/21/Leetcode-143-重排链表/" >Leetcode 143. 重排链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/21</span><a class="archive-post-title" href= "/2020/10/21/Leetcode-925-长按键入/" >Leetcode 925. 长按键入</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/19</span><a class="archive-post-title" href= "/2020/10/19/Leetcode-844-比较含退格的字符串/" >Leetcode 844. 比较含退格的字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/18</span><a class="archive-post-title" href= "/2020/10/18/Leetcode-19-删除链表的倒数第N个节点/" >Leetcode 19. 删除链表的倒数第N个节点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/16</span><a class="archive-post-title" href= "/2020/10/16/Leetcode-977-有序数组的平方/" >Leetcode 977. 有序数组的平方</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/15</span><a class="archive-post-title" href= "/2020/10/15/Leetcode-116-填充每个节点的下一个右侧节点指针/" >Leetcode 116. 填充每个节点的下一个右侧节点指针</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2020/10/14/Leetcode-1002-查找常用字符/" >Leetcode 1002. 查找常用字符</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2020/10/14/Leetcode-24-两两交换链表中的节点/" >Leetcode 24. 两两交换链表中的节点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/12</span><a class="archive-post-title" href= "/2020/10/12/Leetcode-530-二叉搜索树的最小绝对差/" >Leetcode 530. 二叉搜索树的最小绝对差</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/11</span><a class="archive-post-title" href= "/2020/10/11/Leetcode-416-分割等和子集/" >Leetcode 416. 分割等和子集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/10</span><a class="archive-post-title" href= "/2020/10/10/Leetcode-142-环形链表-II/" >Leetcode 142. 环形链表 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/09</span><a class="archive-post-title" href= "/2020/10/09/Leetcode-141-环形链表/" >Leetcode 141. 环形链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/08</span><a class="archive-post-title" href= "/2020/10/08/Leetcode-344-反转字符串/" >Leetcode 344. 反转字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/07</span><a class="archive-post-title" href= "/2020/10/07/Leetcode-75-颜色分类/" >Leetcode 75. 颜色分类</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/05</span><a class="archive-post-title" href= "/2020/10/05/Leetcode-18-四数之和/" >Leetcode 18. 四数之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/03</span><a class="archive-post-title" href= "/2020/10/03/Leetcode-1-两数之和/" >Leetcode 1. 两数之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/02</span><a class="archive-post-title" href= "/2020/10/02/Leetcode-771-宝石与石头/" >Leetcode 771. 宝石与石头</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/02</span><a class="archive-post-title" href= "/2020/10/02/Leetcode-LCP-19-秋叶收藏集/" >Leetcode LCP 19. 秋叶收藏集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/30</span><a class="archive-post-title" href= "/2020/09/30/Leetcode-701-二叉搜索树中的插入操作/" >Leetcode 701. 二叉搜索树中的插入操作</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/29</span><a class="archive-post-title" href= "/2020/09/29/Leetcode-145-二叉树的后序遍历/" >Leetcode 145. 二叉树的后序遍历</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/28</span><a class="archive-post-title" href= "/2020/09/28/Leetcode-117-填充每个节点的下一个右侧节点指针-II/" >Leetcode 117. 填充每个节点的下一个右侧节点指针 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/27</span><a class="archive-post-title" href= "/2020/09/27/Leetcode-235-二叉搜索树的最近公共祖先/" >Leetcode 235. 二叉搜索树的最近公共祖先</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/26</span><a class="archive-post-title" href= "/2020/09/26/Leetcode-113-路径总和-II/" >Leetcode 113. 路径总和 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/25</span><a class="archive-post-title" href= "/2020/09/25/Leetcode-106-从中序与后序遍历序列构造二叉树/" >Leetcode 106. 从中序与后序遍历序列构造二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/24</span><a class="archive-post-title" href= "/2020/09/24/Leetcode-501-二叉搜索树中的众数/" >Leetcode 501. 二叉搜索树中的众数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/2020/09/23/Leetcode-617-合并二叉树/" >Leetcode 617. 合并二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2020/09/22/Leetcode-968-监控二叉树/" >Leetcode 968. 监控二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/21</span><a class="archive-post-title" href= "/2020/09/21/Leetcode-538-把二叉搜索树转换为累加树/" >Leetcode 538. 把二叉搜索树转换为累加树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/20</span><a class="archive-post-title" href= "/2020/09/20/Leetcode-78-子集/" >Leetcode 78. 子集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/19</span><a class="archive-post-title" href= "/2020/09/19/Leetcode-404-左叶子之和/" >Leetcode 404. 左叶子之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/18</span><a class="archive-post-title" href= "/2020/09/18/Leetcode-47-全排列-II/" >Leetcode 47. 全排列 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2020/09/17/Leetcode-685-冗余连接-II/" >Leetcode 685. 冗余连接 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/16</span><a class="archive-post-title" href= "/2020/09/16/Leetcode-226-翻转二叉树/" >Leetcode 226. 翻转二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/15</span><a class="archive-post-title" href= "/2020/09/15/Leetcode-37-解数独/" >Leetcode 37. 解数独</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/14</span><a class="archive-post-title" href= "/2020/09/14/Leetcode-94-二叉树的中序遍历/" >Leetcode 94. 二叉树的中序遍历</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/2020/09/13/Leetcode-79-单词搜索/" >Leetcode 79. 单词搜索</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/12</span><a class="archive-post-title" href= "/2020/09/12/Leetcode-637-二叉树的层平均值/" >Leetcode 637. 二叉树的层平均值</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/11</span><a class="archive-post-title" href= "/2020/09/11/Leetcode-216-组合总和-III/" >Leetcode 216. 组合总和 III</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span><a class="archive-post-title" href= "/2020/09/10/Leetcode-40-组合总和-II/" >Leetcode 40. 组合总和 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span><a class="archive-post-title" href= "/2020/09/09/Leetcode-39-组合总和/" >Leetcode 39. 组合总和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/08</span><a class="archive-post-title" href= "/2020/09/08/Leetcode-77-组合/" >Leetcode 77. 组合</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span><a class="archive-post-title" href= "/2020/09/07/Leetcode-347-前-K-个高频元素/" >Leetcode 347. 前 K 个高频元素</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/06</span><a class="archive-post-title" href= "/2020/09/06/Leetcode-107-二叉树的层次遍历-II/" >Leetcode 107. 二叉树的层次遍历 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span><a class="archive-post-title" href= "/2020/09/05/Leetcode-60-第k个排列/" >Leetcode 60. 第k个排列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/04</span><a class="archive-post-title" href= "/2020/09/04/Leetcode-257-二叉树的所有路径/" >Leetcode 257. 二叉树的所有路径</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/03</span><a class="archive-post-title" href= "/2020/09/03/Leetcode-51-N-皇后/" >Leetcode 51. N 皇后</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/2020/09/02/Leetcode-剑指-Offer-20-表示数值的字符串/" >Leetcode 剑指 Offer 20. 表示数值的字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/01</span><a class="archive-post-title" href= "/2020/09/01/Leetcode-486-预测赢家/" >Leetcode 486. 预测赢家</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span><a class="archive-post-title" href= "/2020/08/31/Leetcode-841-钥匙和房间/" >Leetcode 841. 钥匙和房间</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span><a class="archive-post-title" href= "/2020/08/30/Leetcode-557-反转字符串中的单词-III/" >Leetcode 557. 反转字符串中的单词 III</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/29</span><a class="archive-post-title" href= "/2020/08/29/Leetcode-214-最短回文串/" >Leetcode 214. 最短回文串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/28</span><a class="archive-post-title" href= "/2020/08/28/Leetcode-657-机器人能否返回原点/" >Leetcode 657. 机器人能否返回原点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/27</span><a class="archive-post-title" href= "/2020/08/27/Leetcode-332-重新安排行程/" >Leetcode 332. 重新安排行程</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span><a class="archive-post-title" href= "/2020/08/26/Leetcode-17-电话号码的字母组合/" >Leetcode 17. 电话号码的字母组合</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/25</span><a class="archive-post-title" href= "/2020/08/25/Leetcode-491-递增子序列/" >Leetcode 491. 递增子序列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/24</span><a class="archive-post-title" href= "/2020/08/24/Leetcode-459-重复的子字符串/" >Leetcode 459. 重复的子字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span><a class="archive-post-title" href= "/2020/08/23/Leetcode-201-数字范围按位与/" >Leetcode 201. 数字范围按位与</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/22</span><a class="archive-post-title" href= "/2020/08/22/Leetcode-679-24-点游戏/" >Leetcode 679. 24 点游戏</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/21</span><a class="archive-post-title" href= "/2020/08/21/Leetcode-111-二叉树的最小深度/" >Leetcode 111. 二叉树的最小深度</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/20</span><a class="archive-post-title" href= "/2020/08/20/Leetcode-529-扫雷游戏/" >Leetcode 529. 扫雷游戏</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/19</span><a class="archive-post-title" href= "/2020/08/19/Leetcode-647-回文子串/" >Leetcode 647. 回文子串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/2020/08/18/Leetcode-109-有序链表转换二叉搜索树/" >Leetcode 109. 有序链表转换二叉搜索树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/17</span><a class="archive-post-title" href= "/2020/08/17/Leetcode-110-平衡二叉树/" >Leetcode 110. 平衡二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/16</span><a class="archive-post-title" href= "/2020/08/16/Leetcode-733-图像渲染/" >Leetcode 733. 图像渲染</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/15</span><a class="archive-post-title" href= "/2020/08/15/Leetcode-546-移除盒子/" >Leetcode 546. 移除盒子</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/14</span><a class="archive-post-title" href= "/2020/08/14/Leetcode-20-有效的括号/" >Leetcode 20. 有效的括号</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span><a class="archive-post-title" href= "/2020/08/13/Leetcode-43-字符串相乘/" >Leetcode 43. 字符串相乘</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/12</span><a class="archive-post-title" href= "/2020/08/12/Leetcode-133-克隆图/" >Leetcode 133. 克隆图</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/11</span><a class="archive-post-title" href= "/2020/08/11/Leetcode-130-被围绕的区域/" >Leetcode 130. 被围绕的区域</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/10</span><a class="archive-post-title" href= "/2020/08/10/Leetcode-696-计数二进制子串/" >Leetcode 696. 计数二进制子串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/09</span><a class="archive-post-title" href= "/2020/08/09/Leetcode-93-复原IP地址/" >Leetcode 93. 复原IP地址</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/08</span><a class="archive-post-title" href= "/2020/08/08/Leetcode-99-恢复二叉搜索树/" >Leetcode 99. 恢复二叉搜索树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/07</span><a class="archive-post-title" href= "/2020/08/07/Leetcode-100-相同的树/" >Leetcode 100. 相同的树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/06</span><a class="archive-post-title" href= "/2020/08/06/Leetcode-336-回文对/" >Leetcode 336. 回文对</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/05</span><a class="archive-post-title" href= "/2020/08/05/Leetcode-337-打家劫舍-III/" >Leetcode 337. 打家劫舍 III</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/04</span><a class="archive-post-title" href= "/2020/08/04/Leetcode-207-课程表/" >Leetcode 207. 课程表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/03</span><a class="archive-post-title" href= "/2020/08/03/Leetcode-415-字符串相加/" >Leetcode 415. 字符串相加</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/02</span><a class="archive-post-title" href= "/2020/08/02/Leetcode-114-二叉树展开为链表/" >Leetcode 114. 二叉树展开为链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/02</span><a class="archive-post-title" href= "/2020/08/02/Leetcode-5476-找出数组游戏的赢家/" >Leetcode 5476. 找出数组游戏的赢家</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/02</span><a class="archive-post-title" href= "/2020/08/02/Leetcode-5477-排布二进制网格的最少交换次数/" >Leetcode 5477. 排布二进制网格的最少交换次数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/01</span><a class="archive-post-title" href= "/2020/08/01/Leetcode-632-最小区间/" >Leetcode 632. 最小区间</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/31</span><a class="archive-post-title" href= "/2020/07/31/Leetcode-面试题-08-03-魔术索引/" >Leetcode 面试题 08.03. 魔术索引</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/30</span><a class="archive-post-title" href= "/2020/07/30/Leetcode-343-整数拆分/" >Leetcode 343. 整数拆分</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/29</span><a class="archive-post-title" href= "/2020/07/29/Leetcode-2-两数相加/" >Leetcode 2. 两数相加</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/28</span><a class="archive-post-title" href= "/2020/07/28/Leetcode-104-二叉树的最大深度/" >Leetcode 104. 二叉树的最大深度</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/27</span><a class="archive-post-title" href= "/2020/07/27/Leetcode-392-判断子序列/" >Leetcode 392. 判断子序列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/26</span><a class="archive-post-title" href= "/2020/07/26/Leetcode-329-矩阵中的最长递增路径/" >Leetcode 329. 矩阵中的最长递增路径</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/26</span><a class="archive-post-title" href= "/2020/07/26/Leetcode-5473-灯泡开关-IV/" >Leetcode 5473. 灯泡开关 IV</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/25</span><a class="archive-post-title" href= "/2020/07/25/Leetcode-410-分割数组的最大值/" >Leetcode 410. 分割数组的最大值</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/2020/07/23/Leetcode-64-最小路径和/" >Leetcode 64. 最小路径和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/22</span><a class="archive-post-title" href= "/2020/07/22/Leetcode-剑指-Offer-11-旋转数组的最小数字/" >Leetcode 剑指 Offer 11. 旋转数组的最小数字</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span><a class="archive-post-title" href= "/2020/07/21/Leetcode-95-不同的二叉搜索树-II/" >Leetcode 95. 不同的二叉搜索树 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/20</span><a class="archive-post-title" href= "/2020/07/20/Leetcode-167-两数之和-II-输入有序数组/" >Leetcode 167. 两数之和 II - 输入有序数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/19</span><a class="archive-post-title" href= "/2020/07/19/Leetcode-312-戳气球/" >Leetcode 312. 戳气球</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/18</span><a class="archive-post-title" href= "/2020/07/18/Leetcode-97-交错字符串/" >Leetcode 97. 交错字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/17</span><a class="archive-post-title" href= "/2020/07/17/Leetcode-35-搜索插入位置/" >Leetcode 35. 搜索插入位置</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/16</span><a class="archive-post-title" href= "/2020/07/16/Leetcode-785-判断二分图/" >Leetcode 785. 判断二分图</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/15</span><a class="archive-post-title" href= "/2020/07/15/Leetcode-96-不同的二叉搜索树/" >Leetcode 96. 不同的二叉搜索树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/14</span><a class="archive-post-title" href= "/2020/07/14/Leetcode-120-三角形最小路径和/" >Leetcode 120. 三角形最小路径和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/13</span><a class="archive-post-title" href= "/2020/07/13/Leetcode-350-两个数组的交集-II/" >Leetcode 350. 两个数组的交集 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/12</span><a class="archive-post-title" href= "/2020/07/12/Leetcode-174-地下城游戏/" >Leetcode 174. 地下城游戏</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/2020/07/11/Leetcode-315-计算右侧小于当前元素的个数/" >Leetcode 315. 计算右侧小于当前元素的个数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/10</span><a class="archive-post-title" href= "/2020/07/10/Leetcode-309-最佳买卖股票时机含冷冻期/" >Leetcode 309. 最佳买卖股票时机含冷冻期</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/09</span><a class="archive-post-title" href= "/2020/07/09/Leetcode-面试题-17-13-恢复空格/" >Leetcode 面试题 17.13. 恢复空格</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span><a class="archive-post-title" href= "/2020/07/08/Leetcode-面试题-16-11-跳水板/" >Leetcode 面试题 16.11. 跳水板</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/07</span><a class="archive-post-title" href= "/2020/07/07/Leetcode-112-路径总和/" >Leetcode 112. 路径总和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/06</span><a class="archive-post-title" href= "/2020/07/06/Leetcode-63-不同路径-II/" >Leetcode 63. 不同路径 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/05</span><a class="archive-post-title" href= "/2020/07/05/Leetcode-5454-统计全-1-子矩形/" >Leetcode 5454. 统计全 1 子矩形</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/05</span><a class="archive-post-title" href= "/2020/07/05/Leetcode-44-通配符匹配/" >Leetcode 44. 通配符匹配</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span><a class="archive-post-title" href= "/2020/07/04/Leetcode-32-最长有效括号/" >Leetcode 32. 最长有效括号</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/03</span><a class="archive-post-title" href= "/2020/07/03/Leetcode-108-将有序数组转换为二叉搜索树/" >Leetcode 108. 将有序数组转换为二叉搜索树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span><a class="archive-post-title" href= "/2020/07/02/Leetcode-378-有序矩阵中第K小的元素/" >Leetcode 378. 有序矩阵中第K小的元素</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/01</span><a class="archive-post-title" href= "/2020/07/01/Leetcode-718-最长重复子数组/" >Leetcode 718. 最长重复子数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/30</span><a class="archive-post-title" href= "/2020/06/30/Leetcode-剑指-Offer-09-用两个栈实现队列/" >Leetcode 剑指 Offer 09. 用两个栈实现队列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span><a class="archive-post-title" href= "/2020/06/29/Leetcode-215-数组中的第K个最大元素/" >Leetcode 215. 数组中的第K个最大元素</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span><a class="archive-post-title" href= "/2020/06/28/Leetcode-209-长度最小的子数组/" >Leetcode 209. 长度最小的子数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/27</span><a class="archive-post-title" href= "/2020/06/27/Leetcode-41-缺失的第一个正数/" >Leetcode 41. 缺失的第一个正数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/26</span><a class="archive-post-title" href= "/2020/06/26/Leetcode-面试题-02-01-移除重复节点/" >Leetcode 面试题 02.01. 移除重复节点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/25</span><a class="archive-post-title" href= "/2020/06/25/Leetcode-139-单词拆分/" >Leetcode 139. 单词拆分</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/25</span><a class="archive-post-title" href= "/2020/06/25/图形学知识点/" >图形学知识点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/2020/06/24/Leetcode-16-最接近的三数之和/" >Leetcode 16. 最接近的三数之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/23</span><a class="archive-post-title" href= "/2020/06/23/Leetcode-67-二进制求和/" >Leetcode 67. 二进制求和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/22</span><a class="archive-post-title" href= "/2020/06/22/Leetcode-面试题-16-18-模式匹配/" >Leetcode 面试题 16.18. 模式匹配</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/21</span><a class="archive-post-title" href= "/2020/06/21/Leetcode-124-二叉树中的最大路径和/" >Leetcode 124. 二叉树中的最大路径和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/21</span><a class="archive-post-title" href= "/2020/06/21/嵌入式复习作业汇总/" >嵌入式复习作业汇总</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/20</span><a class="archive-post-title" href= "/2020/06/20/Leetcode-10-正则表达式匹配/" >Leetcode 10. 正则表达式匹配</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/19</span><a class="archive-post-title" href= "/2020/06/19/Leetcode-125-验证回文串/" >Leetcode 125. 验证回文串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/18</span><a class="archive-post-title" href= "/2020/06/18/Leetcode-1028-从先序遍历还原二叉树/" >Leetcode 1028. 从先序遍历还原二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span><a class="archive-post-title" href= "/2020/06/17/Leetcode-1014-最佳观光组合/" >Leetcode 1014. 最佳观光组合</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/16</span><a class="archive-post-title" href= "/2020/06/16/Leetcode-297-二叉树的序列化与反序列化/" >Leetcode 297. 二叉树的序列化与反序列化</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/15</span><a class="archive-post-title" href= "/2020/06/15/Leetcode-14-最长公共前缀/" >Leetcode 14. 最长公共前缀</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/14</span><a class="archive-post-title" href= "/2020/06/14/Leetcode-1300-转变数组后最接近目标值的数组和/" >Leetcode 1300. 转变数组后最接近目标值的数组和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/14</span><a class="archive-post-title" href= "/2020/06/14/Leetcode-5437-不同整数的最少数目/" >Leetcode 5437. 不同整数的最少数目</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/13</span><a class="archive-post-title" href= "/2020/06/13/Leetcode-70-爬楼梯/" >Leetcode 70. 爬楼梯</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/12</span><a class="archive-post-title" href= "/2020/06/12/Leetcode-15-三数之和/" >Leetcode 15. 三数之和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span><a class="archive-post-title" href= "/2020/06/11/Leetcode-739-每日温度/" >Leetcode 739. 每日温度</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/10</span><a class="archive-post-title" href= "/2020/06/10/Leetcode-9-回文数/" >Leetcode 9. 回文数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/09</span><a class="archive-post-title" href= "/2020/06/09/Leetcode-面试题46-把数字翻译成字符串/" >Leetcode 面试题46. 把数字翻译成字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/08</span><a class="archive-post-title" href= "/2020/06/08/Leetcode-990-等式方程的可满足性/" >Leetcode 990. 等式方程的可满足性</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/07</span><a class="archive-post-title" href= "/2020/06/07/Leetcode-5430-设计浏览器历史记录/" >Leetcode 5430. 设计浏览器历史记录</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/07</span><a class="archive-post-title" href= "/2020/06/07/Leetcode-126-单词接龙-II/" >Leetcode 126. 单词接龙 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/06</span><a class="archive-post-title" href= "/2020/06/06/Leetcode-128-最长连续序列/" >Leetcode 128. 最长连续序列</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/05</span><a class="archive-post-title" href= "/2020/06/05/Leetcode-面试题29-顺时针打印矩阵/" >Leetcode 面试题29. 顺时针打印矩阵</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span><a class="archive-post-title" href= "/2020/06/04/Leetcode-238-除自身以外数组的乘积/" >Leetcode 238. 除自身以外数组的乘积</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/03</span><a class="archive-post-title" href= "/2020/06/03/Leetcode-837-新21点/" >Leetcode 837. 新21点</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2020/06/02/Leetcode-面试题64-求1-2-…-n/" >Leetcode 面试题64. 求1+2+…+n</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2020/06/01/Leetcode-1431-拥有最多糖果的孩子/" >Leetcode 1431. 拥有最多糖果的孩子</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/31</span><a class="archive-post-title" href= "/2020/05/31/Leetcode-101-对称二叉树/" >Leetcode 101. 对称二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/31</span><a class="archive-post-title" href= "/2020/05/31/Leetcode-5425-切割后面积最大的蛋糕/" >Leetcode 5425. 切割后面积最大的蛋糕</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span><a class="archive-post-title" href= "/2020/05/30/Leetcode-84-柱状图中最大的矩形/" >Leetcode 84. 柱状图中最大的矩形</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/29</span><a class="archive-post-title" href= "/2020/05/29/Leetcode-198-打家劫舍/" >Leetcode 198. 打家劫舍</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/28</span><a class="archive-post-title" href= "/2020/05/28/Leetcode-394-字符串解码/" >Leetcode 394. 字符串解码</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/27</span><a class="archive-post-title" href= "/2020/05/27/Leetcode-974-和可被-K-整除的子数组/" >Leetcode 974. 和可被 K 整除的子数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/26</span><a class="archive-post-title" href= "/2020/05/26/Leetcode-287-寻找重复数/" >Leetcode 287. 寻找重复数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/25</span><a class="archive-post-title" href= "/2020/05/25/Leetcode-146-LRU缓存机制/" >Leetcode 146. LRU缓存机制</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span><a class="archive-post-title" href= "/2020/05/24/Leetcode-4-寻找两个正序数组的中位数/" >Leetcode 4. 寻找两个正序数组的中位数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span><a class="archive-post-title" href= "/2020/05/24/Leetcode-5417-定长子串中元音的最大数目/" >Leetcode 5417. 定长子串中元音的最大数目</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span><a class="archive-post-title" href= "/2020/05/24/Leetcode-5418-二叉树中的伪回文路径/" >Leetcode 5418. 二叉树中的伪回文路径</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/23</span><a class="archive-post-title" href= "/2020/05/23/Leetcode-76-最小覆盖子串/" >Leetcode 76. 最小覆盖子串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/22</span><a class="archive-post-title" href= "/2020/05/22/Leetcode-105-从前序与中序遍历序列构造二叉树/" >Leetcode 105. 从前序与中序遍历序列构造二叉树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/21</span><a class="archive-post-title" href= "/2020/05/21/Leetcode-5-最长回文子串/" >Leetcode 5. 最长回文子串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/20</span><a class="archive-post-title" href= "/2020/05/20/Leetcode-1371-每个元音包含偶数次的最长子字符串/" >Leetcode 1371. 每个元音包含偶数次的最长子字符串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/19</span><a class="archive-post-title" href= "/2020/05/19/Leetcode-680-验证回文字符串-Ⅱ/" >Leetcode 680. 验证回文字符串 Ⅱ</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/18</span><a class="archive-post-title" href= "/2020/05/18/Leetcode-152-乘积最大子数组/" >Leetcode 152. 乘积最大子数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/17</span><a class="archive-post-title" href= "/2020/05/17/Leetcode-210-课程表-II/" >Leetcode 210. 课程表 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/17</span><a class="archive-post-title" href= "/2020/05/17/Leetcode-5413-重新排列句子中的单词/" >Leetcode 5413. 重新排列句子中的单词</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/16</span><a class="archive-post-title" href= "/2020/05/16/Leetcode-5396-连续字符/" >Leetcode 5396. 连续字符</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/16</span><a class="archive-post-title" href= "/2020/05/16/Leetcode-5397-最简分数/" >Leetcode 5397. 最简分数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/16</span><a class="archive-post-title" href= "/2020/05/16/Leetcode-25-K-个一组翻转链表/" >Leetcode 25. K 个一组翻转链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/15</span><a class="archive-post-title" href= "/2020/05/15/Leetcode-560-和为K的子数组/" >Leetcode 560. 和为K的子数组</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/14</span><a class="archive-post-title" href= "/2020/05/14/Leetcode-136-只出现一次的数字/" >Leetcode 136. 只出现一次的数字</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/13</span><a class="archive-post-title" href= "/2020/05/13/Leetcode-102-二叉树的层序遍历/" >Leetcode 102. 二叉树的层序遍历</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/12</span><a class="archive-post-title" href= "/2020/05/12/Leetcode-155-最小栈/" >Leetcode 155. 最小栈</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/11</span><a class="archive-post-title" href= "/2020/05/11/Leetcode-50-Pow-x-n/" >Leetcode 50. Pow(x, n)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/10</span><a class="archive-post-title" href= "/2020/05/10/Leetcode-236-二叉树的最近公共祖先/" >Leetcode 236. 二叉树的最近公共祖先</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/10</span><a class="archive-post-title" href= "/2020/05/10/Leetcode-5405-形成两个异或相等数组的三元组数目/" >Leetcode 5405. 形成两个异或相等数组的三元组数目</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/09</span><a class="archive-post-title" href= "/2020/05/09/Leetcode-69-x-的平方根/" >Leetcode 69. x 的平方根</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/08</span><a class="archive-post-title" href= "/2020/05/08/Leetcode-221-最大正方形/" >Leetcode 221. 最大正方形</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/07</span><a class="archive-post-title" href= "/2020/05/07/Leetcode-572-另一个树的子树/" >Leetcode 572. 另一个树的子树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/2020/05/06/Leetcode-983-最低票价/" >Leetcode 983. 最低票价</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/05</span><a class="archive-post-title" href= "/2020/05/05/Leetcode-98-验证二叉搜索树/" >Leetcode 98. 验证二叉搜索树</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/04</span><a class="archive-post-title" href= "/2020/05/04/Leetcode-45-跳跃游戏-II/" >Leetcode 45. 跳跃游戏 II</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/03</span><a class="archive-post-title" href= "/2020/05/03/王道数据结构-线性表的顺序表示-课后算法题/" >王道数据结构 线性表的顺序表示(课后算法题)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/03</span><a class="archive-post-title" href= "/2020/05/03/Leetcode-53-最大子序和/" >Leetcode 53. 最大子序和</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/02</span><a class="archive-post-title" href= "/2020/05/02/Leetcode-3-无重复字符的最长子串/" >Leetcode 3. 无重复字符的最长子串</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/01</span><a class="archive-post-title" href= "/2020/05/01/Leetcode-21-合并两个有序链表/" >Leetcode 21. 合并两个有序链表</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/30</span><a class="archive-post-title" href= "/2020/04/30/Leetcode-202-快乐数/" >Leetcode 202. 快乐数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/29</span><a class="archive-post-title" href= "/2020/04/29/Leetcode-1025-除数博弈/" >Leetcode 1025. 除数博弈</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/10</span><a class="archive-post-title" href= "/2020/04/10/2021考研计划/" >2021考研计划</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/24</span><a class="archive-post-title" href= "/2020/02/24/找位置问题/" >找位置问题</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/东野圭吾《信》/" >东野圭吾《信》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/东野圭吾《秘密》/" >东野圭吾《秘密》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/东野圭吾《新参者》/" >东野圭吾《新参者》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/东野圭吾《红手指》/" >东野圭吾《红手指》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/孙沁文《凛冬之棺》/" >孙沁文《凛冬之棺》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/时晨《五行塔事件》/" >时晨《五行塔事件》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/时晨《傀儡村事件》/" >时晨《傀儡村事件》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/李若楠《我的温柔是锋芒》/" >李若楠《我的温柔是锋芒》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/时晨《镜狱岛事件》/" >时晨《镜狱岛事件》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/陆烨华《超能力侦探事务所》/" >陆烨华《超能力侦探事务所》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/14</span><a class="archive-post-title" href= "/2019/10/14/陆烨华《超能力侦探事务所2-神秘组织》/" >陆烨华《超能力侦探事务所2:神秘组织》</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/14</span><a class="archive-post-title" href= "/2019/07/14/博客初步搭建成功/" >博客初步搭建成功</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="哈希表"><span class="iconfont-archer">&#xe606;</span>哈希表</span>
    
        <span class="sidebar-tag-name" data-tags="树"><span class="iconfont-archer">&#xe606;</span>树</span>
    
        <span class="sidebar-tag-name" data-tags="深度优先搜索"><span class="iconfont-archer">&#xe606;</span>深度优先搜索</span>
    
        <span class="sidebar-tag-name" data-tags="动态规划"><span class="iconfont-archer">&#xe606;</span>动态规划</span>
    
        <span class="sidebar-tag-name" data-tags="数组"><span class="iconfont-archer">&#xe606;</span>数组</span>
    
        <span class="sidebar-tag-name" data-tags="深度优先遍历"><span class="iconfont-archer">&#xe606;</span>深度优先遍历</span>
    
        <span class="sidebar-tag-name" data-tags="栈"><span class="iconfont-archer">&#xe606;</span>栈</span>
    
        <span class="sidebar-tag-name" data-tags="广度优先搜索"><span class="iconfont-archer">&#xe606;</span>广度优先搜索</span>
    
        <span class="sidebar-tag-name" data-tags="回溯"><span class="iconfont-archer">&#xe606;</span>回溯</span>
    
        <span class="sidebar-tag-name" data-tags="双指针"><span class="iconfont-archer">&#xe606;</span>双指针</span>
    
        <span class="sidebar-tag-name" data-tags="字符串"><span class="iconfont-archer">&#xe606;</span>字符串</span>
    
        <span class="sidebar-tag-name" data-tags="图"><span class="iconfont-archer">&#xe606;</span>图</span>
    
        <span class="sidebar-tag-name" data-tags="广度优先遍历"><span class="iconfont-archer">&#xe606;</span>广度优先遍历</span>
    
        <span class="sidebar-tag-name" data-tags="二分查找"><span class="iconfont-archer">&#xe606;</span>二分查找</span>
    
        <span class="sidebar-tag-name" data-tags="贪心"><span class="iconfont-archer">&#xe606;</span>贪心</span>
    
        <span class="sidebar-tag-name" data-tags="数学"><span class="iconfont-archer">&#xe606;</span>数学</span>
    
        <span class="sidebar-tag-name" data-tags="链表"><span class="iconfont-archer">&#xe606;</span>链表</span>
    
        <span class="sidebar-tag-name" data-tags="双向链表"><span class="iconfont-archer">&#xe606;</span>双向链表</span>
    
        <span class="sidebar-tag-name" data-tags="拓扑排序"><span class="iconfont-archer">&#xe606;</span>拓扑排序</span>
    
        <span class="sidebar-tag-name" data-tags="KMP"><span class="iconfont-archer">&#xe606;</span>KMP</span>
    
        <span class="sidebar-tag-name" data-tags="滑动窗口"><span class="iconfont-archer">&#xe606;</span>滑动窗口</span>
    
        <span class="sidebar-tag-name" data-tags="堆"><span class="iconfont-archer">&#xe606;</span>堆</span>
    
        <span class="sidebar-tag-name" data-tags="字典树"><span class="iconfont-archer">&#xe606;</span>字典树</span>
    
        <span class="sidebar-tag-name" data-tags="并查集"><span class="iconfont-archer">&#xe606;</span>并查集</span>
    
        <span class="sidebar-tag-name" data-tags="递归"><span class="iconfont-archer">&#xe606;</span>递归</span>
    
        <span class="sidebar-tag-name" data-tags="吴恩达团队NLP"><span class="iconfont-archer">&#xe606;</span>吴恩达团队NLP</span>
    
        <span class="sidebar-tag-name" data-tags="吴恩达深度学习"><span class="iconfont-archer">&#xe606;</span>吴恩达深度学习</span>
    
        <span class="sidebar-tag-name" data-tags="线性表"><span class="iconfont-archer">&#xe606;</span>线性表</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="杂谈"><span class="iconfont-archer">&#xe60a;</span>杂谈</span>
    
        <span class="sidebar-category-name" data-categories="算法相关"><span class="iconfont-archer">&#xe60a;</span>算法相关</span>
    
        <span class="sidebar-category-name" data-categories="PAT"><span class="iconfont-archer">&#xe60a;</span>PAT</span>
    
        <span class="sidebar-category-name" data-categories="2019推理小说阅读计划"><span class="iconfont-archer">&#xe60a;</span>2019推理小说阅读计划</span>
    
        <span class="sidebar-category-name" data-categories="NLP"><span class="iconfont-archer">&#xe60a;</span>NLP</span>
    
        <span class="sidebar-category-name" data-categories="深度学习"><span class="iconfont-archer">&#xe60a;</span>深度学习</span>
    
        <span class="sidebar-category-name" data-categories="大三课程学习"><span class="iconfont-archer">&#xe60a;</span>大三课程学习</span>
    
        <span class="sidebar-category-name" data-categories="大四课程学习"><span class="iconfont-archer">&#xe60a;</span>大四课程学习</span>
    
        <span class="sidebar-category-name" data-categories="浙工商OJ"><span class="iconfont-archer">&#xe60a;</span>浙工商OJ</span>
    
        <span class="sidebar-category-name" data-categories="牛客网"><span class="iconfont-archer">&#xe60a;</span>牛客网</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "John Doe"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


